# analisa_novo_evento.py
import io, json, re, time
from pathlib import Path
from typing import List

import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

# ==== PDF / DOCX readers (tenta PyMuPDF; cai para pdfminer se faltar) ====
try:
    import fitz  # PyMuPDF
    HAVE_PYMUPDF = True
except Exception:
    HAVE_PYMUPDF = False

try:
    from pdfminer.high_level import extract_text as pdfminer_extract
    HAVE_PDFMINER = True
except Exception:
    HAVE_PDFMINER = False

from docx import Document
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# -----------------------------------------------------------------------------
# CONFIG
# -----------------------------------------------------------------------------
st.set_page_config(page_title="An√°lise de Eventos: WS ‚Ä¢ Precursores ‚Ä¢ TaxonomiaCP", layout="wide")

# Caminho FIXO dos artefatos no GitHub (RAW)
REMOTE_BASE = "https://raw.githubusercontent.com/titetodesco/VisualizarPrecSinaisFracosReports/main"

# Modelo de embeddings (o mesmo usado no preparo)
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# Limiar padr√£o
DEFAULT_THR = 0.50

# -----------------------------------------------------------------------------
# HELPERS (normaliza√ß√£o, leitura, embeddings, utilit√°rios)
# -----------------------------------------------------------------------------
WS_PAREN_RE = re.compile(r"\s*\((?:0\.\d+|1\.0+)\)\s*$")
def clean_ws_name(s: str) -> str:
    """Remove ' (0.53)' do final do WeakSignal, mantendo apenas o texto."""
    if not isinstance(s, str): return ""
    return WS_PAREN_RE.sub("", s).strip()

def _canon(s: str) -> str:
    """Normaliza string para compara√ß√£o: min√∫sculas, sem espa√ßos/h√≠fens/underscores."""
    if not isinstance(s, str):
        return ""
    return re.sub(r"[\s_\-]+", "", s.strip().lower())

def normalize_taxonomia_cols(df: pd.DataFrame) -> pd.DataFrame:
    """
    Aceita varia√ß√µes como 'Dimens√£o', 'Sub-fator', 'terms', 'termos', etc.
    Garante colunas-padr√£o: Dimensao, Fator, Subfator, _termos, _text
    """
    if df is None or df.empty:
        return df

    colmap = {}
    canon = {c: _canon(c) for c in df.columns}

    # Dimensao
    alvo = None
    for c, k in canon.items():
        if k.startswith("dimens"):  # "dimensao", "dimens√£o"
            alvo = c; break
    if alvo: colmap[alvo] = "Dimensao"

    # Fator
    alvo = None
    for c, k in canon.items():
        if k == "fator" or k == "factor":
            alvo = c; break
    if alvo: colmap[alvo] = "Fator"

    # Subfator
    alvo = None
    for c, k in canon.items():
        if "sub" in k and ("fator" in k or "factor" in k):
            alvo = c; break
    if alvo: colmap[alvo] = "Subfator"

    # _termos (pode vir como "termos", "termo", "terms", "term")
    alvo = None
    for c, k in canon.items():
        if k in {"_termos","termos","termo","terms","term"}:
            alvo = c; break
    if alvo: colmap[alvo] = "_termos"

    df = df.rename(columns=colmap)

    # Completa hierarquia vazia (evita quebrar visualiza√ß√µes)
    for needed in ["Dimensao", "Fator", "Subfator"]:
        if needed not in df.columns:
            df[needed] = ""

    # Garante _termos e _text
    if "_termos" not in df.columns:
        df["_termos"] = (df.get("Dimensao","").astype(str) + " " +
                         df.get("Fator","").astype(str) + " " +
                         df.get("Subfator","").astype(str)).str.strip()
    if "_text" not in df.columns:
        df["_text"] = df["_termos"].astype(str)

    return df

def has_embedding_cols(df: pd.DataFrame) -> bool:
    """Retorna True se existir pelo menos uma coluna 'e_0' (ou prefixo e_)."""
    return any(c.startswith("e_") for c in df.columns)

def load_artifact_df(name: str) -> pd.DataFrame:
    """L√™ parquet remoto (GitHub RAW) com pyarrow."""
    url = f"{REMOTE_BASE}/{name}"
    return pd.read_parquet(url, engine="pyarrow")

def load_meta() -> dict:
    url = f"{REMOTE_BASE}/meta.json"
    # l√™ como texto -> json
    meta_txt = pd.read_json(url, typ="series").to_json()
    return json.loads(meta_txt)

def read_pdf_bytes(file_bytes: bytes) -> str:
    """Leitura robusta de PDF (PyMuPDF se dispon√≠vel; sen√£o pdfminer)."""
    if HAVE_PYMUPDF:
        try:
            parts = []
            with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                for page in doc:
                    parts.append(page.get_text("text"))
            return "\n".join(parts)
        except Exception:
            pass
    if HAVE_PDFMINER:
        try:
            return pdfminer_extract(io.BytesIO(file_bytes))
        except Exception:
            pass
    st.error("Nenhum leitor de PDF dispon√≠vel. Instale `PyMuPDF` (prefer√≠vel) ou `pdfminer.six`.")
    return ""

def read_docx(file_bytes: bytes) -> str:
    f = io.BytesIO(file_bytes)
    doc = Document(f)
    paras = []
    for p in doc.paragraphs:
        t = p.text.strip()
        if t:
            paras.append(t)
    return "\n".join(paras)

def to_paragraphs(raw_text: str, min_len=25) -> List[tuple[int,str]]:
    """Quebra o texto em blocos (par√°grafos) minimamente longos."""
    chunks, buf = [], []
    for line in raw_text.splitlines():
        line = line.strip()
        if not line:
            if buf:
                block = " ".join(buf).strip()
                if len(block) >= min_len:
                    chunks.append(block)
                buf = []
        else:
            buf.append(line)
    if buf:
        block = " ".join(buf).strip()
        if len(block) >= min_len:
            chunks.append(block)
    return [(i+1, ch) for i, ch in enumerate(chunks)]

@st.cache_resource(show_spinner=False)
def load_model(name: str):
    return SentenceTransformer(name)

def embed_texts(model, texts: List[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, 384), dtype=np.float32)  # MiniLM-L6-v2 -> 384 dims
    return model.encode(texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)

def emb_matrix(df: pd.DataFrame) -> np.ndarray:
    cols = [c for c in df.columns if c.startswith("e_")]
    M = df[cols].to_numpy(dtype=np.float32)
    norms = np.linalg.norm(M, axis=1, keepdims=True) + 1e-9
    return (M / norms).astype(np.float32)

def to_excel_bytes(dfs: dict[str, pd.DataFrame]) -> bytes:
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine="xlsxwriter") as w:
        for sheet, d in dfs.items():
            d.to_excel(w, sheet_name=sheet[:31], index=False)
    bio.seek(0)
    return bio.read()

def stack_matches(sims: np.ndarray, cand_df: pd.DataFrame, label_cols: list[str], thr: float) -> pd.DataFrame:
    """
    sims: (n_paragraphs x n_candidates)
    cand_df: DataFrame com colunas label_cols (ex.: ["WeakSignal"])
    thr: limiar
    Retorna: DataFrame com colunas ["idx_par","Similarity"] + label_cols (mesmo se vazio)
    """
    hits = np.where(sims >= thr)

    # Sem hits? Retorne DF vazio com colunas esperadas (evita KeyError no sort_values)
    if hits[0].size == 0:
        cols = ["idx_par", "Similarity"] + label_cols
        return pd.DataFrame(columns=cols)

    rows = []
    for i, j in zip(*hits):
        r = {"idx_par": int(i), "Similarity": float(sims[i, j])}
        for c in label_cols:
            r[c] = cand_df.iloc[j][c]
        rows.append(r)

    df = pd.DataFrame(rows)
    return df.sort_values("Similarity", ascending=False).reset_index(drop=True)


# -----------------------------------------------------------------------------
# CARREGA ARTEFATOS (remoto)
# -----------------------------------------------------------------------------
st.title("üß≠ An√°lise de Eventos: Weak Signals ‚Ä¢ Precursores (HTO) ‚Ä¢ TaxonomiaCP")

with st.spinner("Carregando artefatos (embeddings de dicion√°rios e mapa)‚Ä¶"):
    emb_ws   = load_artifact_df("emb_weaksignals.parquet")
    emb_prec = load_artifact_df("emb_precursores.parquet")
    emb_tax  = load_artifact_df("emb_taxonomia.parquet")
    emb_map  = load_artifact_df("emb_mapatriplo.parquet")
    meta     = load_meta()

# Normaliza√ß√µes leves nos artefatos
if "_text" not in emb_ws.columns:
    # tenta achar uma coluna de texto principal
    cand = [c for c in emb_ws.columns if c.lower() in {"_text","weaksignal","term","termo","termos"}]
    if cand:
        emb_ws = emb_ws.rename(columns={cand[0]: "_text"})
    else:
        st.error("Artefato 'weaksignals' sem coluna de texto principal. Refa√ßa o preparo.")
        st.stop()

if "_text" not in emb_prec.columns:
    if "Precursor" in emb_prec.columns:
        emb_prec["_text"] = emb_prec["Precursor"].astype(str)
    else:
        st.error("Artefato 'precursores' sem coluna 'Precursor'. Refa√ßa o preparo.")
        st.stop()
if "HTO" not in emb_prec.columns:
    emb_prec["HTO"] = ""  # evita quebrar

def _norm_tax_headers(df: pd.DataFrame) -> pd.DataFrame:
    """Normaliza cabe√ßalhos vindos do parquet/Excel para: Dimensao, Fator, Subfator, _termos."""
    rename_map = {}
    for c in df.columns:
        cl = c.strip().lower()
        if cl in {"dimens√£o", "dimensao"}:            rename_map[c] = "Dimensao"
        elif cl in {"fatores", "fator"}:              rename_map[c] = "Fator"
        elif cl in {"subfator", "subfator 1"}:        rename_map[c] = "Subfator"
        elif cl in {"_termos","termos","bag de termos","bag of terms"}:
            rename_map[c] = "_termos"
        elif c == "_text":                            rename_map[c] = "_text"
    df = df.rename(columns=rename_map)
    for col in ["Dimensao","Fator","Subfator","_termos"]:
        if col not in df.columns:
            df[col] = np.nan
    return df

# emb_tax = normalize_taxonomia_cols(emb_tax)
emb_tax = _norm_tax_headers(emb_tax)

if "_text" not in emb_map.columns and "Text" in emb_map.columns:
    emb_map["_text"] = emb_map["Text"].astype(str)

# Garante que h√° colunas de embeddings
probs = []
for name, df in [("weaksignals", emb_ws), ("precursores", emb_prec), ("taxonomia", emb_tax), ("mapatriplo", emb_map)]:
    if not has_embedding_cols(df):
        probs.append(name)
if probs:
    st.error("Artefatos sem colunas de embeddings (e_0, e_1, ...): " + ", ".join(probs))
    st.stop()

# Monta matrizes
M_ws   = emb_matrix(emb_ws)
M_prec = emb_matrix(emb_prec)
M_tax  = emb_matrix(emb_tax)
M_map  = emb_matrix(emb_map)

# -----------------------------------------------------------------------------
# SIDEBAR ‚Äì par√¢metros
# -----------------------------------------------------------------------------
st.sidebar.header("Par√¢metros")
thr_ws   = st.sidebar.slider("Limiar (Weak Signals)", 0.0, 0.95, DEFAULT_THR, 0.01)
thr_prec = st.sidebar.slider("Limiar (Precursores)", 0.0, 0.95, DEFAULT_THR, 0.01)
thr_tax  = st.sidebar.slider("Limiar (TaxonomiaCP)", 0.0, 0.95, 0.55, 0.01)
topk_sim_reports = st.sidebar.slider("Top-N relat√≥rios similares", 3, 20, 8, 1)

# -----------------------------------------------------------------------------
# UPLOAD DE ARQUIVOS
# -----------------------------------------------------------------------------
st.subheader("üìé Fa√ßa upload do(s) documento(s) do evento (PDF ou DOCX)")
files = st.file_uploader("Arraste e solte aqui‚Ä¶", type=["pdf","docx"], accept_multiple_files=True)

if not files:
    st.info("Carregue pelo menos um arquivo para iniciar a an√°lise.")
    st.stop()

# -----------------------------------------------------------------------------
# PROCESSA ARQUIVOS (par√°grafos + embeddings)
# -----------------------------------------------------------------------------
model = load_model(MODEL_NAME)

all_rows = []
with st.spinner("Lendo e extraindo texto‚Ä¶"):
    for f in files:
        name = f.name
        data = f.read()
        if name.lower().endswith(".pdf"):
            raw = read_pdf_bytes(data)
        else:
            raw = read_docx(data)
        paras = to_paragraphs(raw, min_len=25)
        for par_id, text in paras:
            all_rows.append({"File": name, "Paragraph": par_id, "Text": text})

df_paras = pd.DataFrame(all_rows)
if df_paras.empty:
    st.warning("N√£o foram encontrados par√°grafos v√°lidos nos arquivos.")
    st.stop()

with st.spinner("Gerando embeddings dos par√°grafos‚Ä¶"):
    E_doc = embed_texts(model, df_paras["Text"].astype(str).tolist())

# -----------------------------------------------------------------------------
# MATCHING (WS, Precursores, Taxonomia, Relat√≥rios similares)
# -----------------------------------------------------------------------------
with st.spinner("Calculando similaridades‚Ä¶"):
    # Weak Signals
    S_ws = cosine_similarity(E_doc, M_ws)  # (P x W)
    ws_hits = stack_matches(S_ws, emb_ws.rename(columns={"_text":"WeakSignal"}), ["WeakSignal"], thr_ws)
    ws_hits["WeakSignal_clean"] = ws_hits["WeakSignal"].map(clean_ws_name)

    # Precursores
    S_prec = cosine_similarity(E_doc, M_prec)  # (P x Pprec)
    prec_hits = stack_matches(S_prec, emb_prec, ["Precursor","HTO"], thr_prec)

    # Taxonomia
    S_tax = cosine_similarity(E_doc, M_tax)  # (P x T)
    tax_hits = stack_matches(S_tax, emb_tax, ["Dimensao","Fator","Subfator","_termos"], thr_tax)

    # Relat√≥rios similares (usa texto dos par√°grafos vs MapaTriplo.Text)
    S_map = cosine_similarity(E_doc, M_map)   # (P x MapRows)
    sim_map_max = S_map.max(axis=0)
    emb_map_reports = emb_map[["Report"]].copy()
    emb_map_reports["max_sim"] = sim_map_max
    sim_reports = (emb_map_reports.groupby("Report", as_index=False)
                   .agg(MaxSim=("max_sim","max"), MeanSim=("max_sim","mean"))
                   .sort_values(["MaxSim","MeanSim"], ascending=False)
                   .head(topk_sim_reports))

def attach_context(df_hits: pd.DataFrame, df_pars: pd.DataFrame) -> pd.DataFrame:
    if df_hits.empty:
        return df_hits
    out = df_hits.merge(
        df_pars.reset_index(drop=True).reset_index().rename(columns={"index":"idx_par"}),
        on="idx_par", how="left"
    )
    return out.rename(columns={"Text":"Trecho"})

ws_hits   = attach_context(ws_hits, df_paras)
prec_hits = attach_context(prec_hits, df_paras)
tax_hits  = attach_context(tax_hits, df_paras)

# ------------------------------------------------------------------
# VISUALIZA√á√ÉO (√önica) ‚Äî WS, Precursores, Taxonomia (com saneamento)
# ------------------------------------------------------------------
st.success(f"Documentos processados: **{df_paras['File'].nunique()}** | Par√°grafos: **{len(df_paras)}**")
c1, c2, c3 = st.columns(3)
with c1: st.metric("Weak Signals (hits)", len(ws_hits))
with c2: st.metric("Precursores (hits)", len(prec_hits))
with c3: st.metric("TaxonomiaCP (hits)", len(tax_hits))

# ----------------- WS -----------------
st.subheader("üîé Weak Signals encontrados")
if ws_hits.empty:
    st.info("Nenhum Weak Signal acima do limiar.")
else:
    ws_freq = (ws_hits.groupby("WeakSignal_clean", as_index=False)
               .agg(Frequencia=("idx_par","count"))
               .sort_values("Frequencia", ascending=False))
    st.dataframe(ws_freq, use_container_width=True)
    st.dataframe(ws_hits[["WeakSignal","Similarity","File","Paragraph","Trecho"]]
                 .sort_values("Similarity", ascending=False)
                 .head(200), use_container_width=True)

# -------------- PRECURSORES --------------
st.subheader("üß© Precursores (HTO) encontrados")
if prec_hits.empty:
    st.info("Nenhum Precursor acima do limiar.")
else:
    prec_freq = (prec_hits.groupby(["HTO","Precursor"], as_index=False)
                 .agg(Frequencia=("idx_par","count"))
                 .sort_values(["HTO","Frequencia"], ascending=[True,False]))
    st.dataframe(prec_freq, use_container_width=True)
    st.dataframe(prec_hits[["HTO","Precursor","Similarity","File","Paragraph","Trecho"]]
                 .sort_values("Similarity", ascending=False)
                 .head(200), use_container_width=True)

# ---------------- TAXONOMIA ----------------
st.markdown("## üß© Visualiza√ß√µes ‚Äî TaxonomiaCP (Dimens√£o ‚Üí Fator ‚Üí Subfator)")

if tax_hits.empty:
    st.info("Nenhum fator da Taxonomia acima do limiar.")
else:
    # 1) normaliza headers e valores
    tax_hits = _norm_tax_headers(tax_hits)
    for col in ["Dimensao","Fator","Subfator","_termos"]:
        tax_hits[col] = (tax_hits[col].astype(str)
                         .str.strip()
                         .replace({"": np.nan, "None": np.nan, "nan": np.nan}))

    # 2) reconstr√≥i Fator ausente a partir do parquet (Subfator ‚Üí Fator)
    sub2fac = (emb_tax[["Subfator","Fator"]]
               .dropna()
               .drop_duplicates())
    sub2fac_map = dict(zip(sub2fac["Subfator"], sub2fac["Fator"]))

    tax_hits["Fator"] = tax_hits["Fator"].fillna(tax_hits["Subfator"].map(sub2fac_map))
    tax_hits["Dimensao"] = tax_hits["Dimensao"].fillna("‚Äî")
    tax_hits["Fator"]    = tax_hits["Fator"].fillna("‚Äî")
    tax_hits["Subfator"] = tax_hits["Subfator"].fillna("‚Äî")

    # 3) Tabela √öNICA de frequ√™ncia (Dimens√£o/Fator/Subfator)
    tax_freq = (tax_hits.groupby(["Dimensao","Fator","Subfator"], as_index=False)
                .agg(Frequencia=("idx_par","count"))
                .sort_values(["Dimensao","Fator","Frequencia"], ascending=[True,True,False]))

    
    st.subheader("üìö TaxonomiaCP (Dimens√£o/Fator/Subfator) encontrados")

if tax_hits.empty:
    st.info("Nenhum fator da Taxonomia acima do limiar.")
else:
    # Normaliza cabe√ßalhos e valores
    tax_hits = _norm_tax_headers(tax_hits).copy()
    for col in ["Dimensao","Fator","Subfator","_termos"]:
        tax_hits[col] = (tax_hits[col].astype(str)
                         .str.strip()
                         .replace({"": np.nan, "None": np.nan, "nan": np.nan}))

    # --- mapeia Subfator -> Fator e Subfator -> Dimensao usando o parquet (fonte da verdade)
    sub2fac_map = (emb_tax[["Subfator","Fator"]]
                   .dropna(subset=["Subfator"])
                   .drop_duplicates()
                   .set_index("Subfator")["Fator"].to_dict())

    sub2dim_map = (emb_tax[["Subfator","Dimensao"]]
                   .dropna(subset=["Subfator"])
                   .drop_duplicates()
                   .set_index("Subfator")["Dimensao"].to_dict())

    # --- Reconstr√≥i Fator/Dimensao quando vierem nulos nos hits (ocorre em match por termo)
    tax_hits["Fator"]    = tax_hits["Fator"].fillna(tax_hits["Subfator"].map(sub2fac_map))
    tax_hits["Dimensao"] = tax_hits["Dimensao"].fillna(tax_hits["Subfator"].map(sub2dim_map))

    # Preenche qualquer resto que tenha ficado vazio (depois do mapeamento)
    tax_hits["Fator"]    = tax_hits["Fator"].fillna("‚Äî")
    tax_hits["Dimensao"] = tax_hits["Dimensao"].fillna("‚Äî")
    tax_hits["Subfator"] = tax_hits["Subfator"].fillna("‚Äî")

    # Base saneada para tudo
    tax_hits_norm = tax_hits.copy()

    # Frequ√™ncia √öNICA (sempre no trio Dimensao/Fator/Subfator)
    tax_freq = (tax_hits_norm.groupby(["Dimensao","Fator","Subfator"], as_index=False)
                .agg(Frequencia=("idx_par","count"))
                .sort_values(["Dimensao","Fator","Frequencia"], ascending=[True,True,False]))

    st.dataframe(tax_freq, use_container_width=True)

    # Amostra dos matches
    st.dataframe(
        tax_hits_norm[["Dimensao","Fator","Subfator","_termos","Similarity","File","Paragraph","Trecho"]]
        .sort_values("Similarity", ascending=False)
        .head(200),
        use_container_width=True
    )

    # A partir daqui, seus gr√°ficos podem usar tax_hits_norm
    tax_plot = tax_hits_norm.copy()
    tax_plot["value"] = 1

    # Prote√ß√£o: se todo mundo vira "‚Äî", evita erro do Plotly
    if tax_plot[["Dimensao","Fator","Subfator"]].nunique().sum() <= 3:
        st.info("Taxonomia com muitos campos vazios. Ajuste os limiares ou verifique os dados.")
    else:
        st.subheader("üå≥ Treemap (Dimens√£o ‚Üí Fator ‚Üí Subfator)")
        fig_tax_tree = px.treemap(
            tax_plot,
            path=["Dimensao","Fator","Subfator"],
            values="value",
            hover_data=["_termos","Similarity","File"],
            title="Treemap da TaxonomiaCP encontrada"
        )
        st.plotly_chart(fig_tax_tree, use_container_width=True)
        

        st.subheader("üåû Sunburst (Dimens√£o ‚Üí Fator ‚Üí Subfator)")
        fig_tax_sun = px.sunburst(
            tax_plot, path=["Dimensao","Fator","Subfator"],
            values="value",
            hover_data=["_termos","Similarity","File"],
            title="Sunburst da TaxonomiaCP encontrada"
        )
        st.plotly_chart(fig_tax_sun, use_container_width=True)

        st.subheader("üè∑Ô∏è Top Subfatores por frequ√™ncia")
        sub_rank = (tax_plot.groupby(["Dimensao","Fator","Subfator"], as_index=False)
                    .agg(Frequencia=("value","sum"))
                    .sort_values("Frequencia", ascending=False)
                    .head(20))
        fig_sub_bar = px.bar(
            sub_rank, x="Frequencia", y="Subfator",
            color="Dimensao", orientation="h",
            hover_data=["Fator"],
            title="Top Subfatores (doc atual)"
        )
        st.plotly_chart(fig_sub_bar, use_container_width=True)

        st.subheader("üî• Heatmap ‚Äî Dimens√£o √ó Fator")
        df_hm = (tax_plot.groupby(["Dimensao","Fator"], as_index=False)
                 .agg(Qtd=("Subfator","nunique")))
        mat_tax = (df_hm
                   .pivot(index="Fator", columns="Dimensao", values="Qtd")
                   .fillna(0).astype(int))
        if not mat_tax.empty:
            fig_tax_hm = px.imshow(
                mat_tax.values,
                labels=dict(x="Dimens√£o", y="Fator", color="Qtd Subfatores"),
                x=mat_tax.columns.tolist(),
                y=mat_tax.index.tolist(),
                title="Qtd de Subfatores por Dimens√£o √ó Fator"
            )
            st.plotly_chart(fig_tax_hm, use_container_width=True)

# ---------------- RELAT√ìRIOS SIMILARES ----------------
st.subheader("üóÇÔ∏è Relat√≥rios pregressos mais similares")
if sim_reports.empty:
    st.info("Sem similares acima de 0.")
else:
    st.dataframe(sim_reports, use_container_width=True)

# ================================
# üå≥ √ÅRVORE ‚Äî HTO ‚Üí Precursor ‚Üí Weak Signal (via embeddings)
# ================================
from streamlit_echarts import st_echarts
import plotly.express as px

st.markdown("## üå≥ √Årvore: HTO ‚Üí Precursores ‚Üí Weak Signals")

# --- 1) Monta tr√≠ades a partir dos hits por par√°grafo
# ws: pega WS limpos, por par√°grafo
_ws = (ws_hits[["idx_par","WeakSignal","Similarity","File","Paragraph","Snippet"]]
       .dropna(subset=["idx_par"])
       .copy())
_ws["WeakSignal_clean"] = _ws["WeakSignal"].map(clean_ws_name)

_ws = _ws[["idx_par","WeakSignal_clean","File","Paragraph","Snippet"]].drop_duplicates()

# precursores por par√°grafo
_prec = (prec_hits[["idx_par","HTO","Precursor"]]
         .dropna(subset=["idx_par","HTO","Precursor"])
         .drop_duplicates())

# tr√≠ades: HTO/Precursor + WeakSignal no mesmo par√°grafo
tri = _prec.merge(_ws, on="idx_par", how="inner")
tri.rename(columns={"WeakSignal_clean":"WeakSignal"}, inplace=True)

if tri.empty:
    st.info("Sem tr√≠ades HTO‚ÄìPrecursor‚ÄìWeak Signal com os limiares atuais.")
else:
    # --- 2) Filtros (por arquivo e frequ√™ncia m√≠nima por WS dentro do precursor)
    cA, cB, cC = st.columns([2,2,1])
    with cA:
        files_sel = st.multiselect(
            "Filtrar por arquivo (upload)",
            sorted(tri["File"].dropna().unique().tolist()),
            default=None
        )
    with cB:
        min_freq = st.number_input("Frequ√™ncia m√≠nima (WS por precursor)", 1, 100, 1, 1)
    with cC:
        init_depth = st.slider("Profundidade inicial", 1, 3, 2)

    tri_f = tri.copy()
    if files_sel:
        tri_f = tri_f[tri_f["File"].isin(files_sel)]

    # aplica corte por frequ√™ncia m√≠nima no n√≠vel WS dentro de cada precursor
    if not tri_f.empty:
        grp = (tri_f.groupby(["HTO","Precursor","WeakSignal"], as_index=False)
                     .agg(Freq=("idx_par","count")))
        keep = grp[grp["Freq"] >= int(min_freq)][["HTO","Precursor","WeakSignal"]]
        tri_f = tri_f.merge(keep, on=["HTO","Precursor","WeakSignal"], how="inner")

    if tri_f.empty:
        st.info("Sem dados ap√≥s os filtros atuais.")
    else:
        # --- 3) √çndice para drill-down e estrutura da √°rvore (ECharts)
        node_index = {}  # chave_de_no -> set de √≠ndices (linhas) em tri_f

        def add_index(key, rows_idx):
            node_index.setdefault(key, set()).update(rows_idx)

        # constr√≥i dicion√°rio hier√°rquico {HTO: {Precursor: {WS: [idx]}}}
        tree_dict = {}
        for i, r in tri_f.reset_index(drop=True).iterrows():
            h, p, w = str(r["HTO"]), str(r["Precursor"]), str(r["WeakSignal"])
            tree_dict.setdefault(h, {}).setdefault(p, {}).setdefault(w, []).append(i)

        # remove ramos vazios (j√° deve estar tratado, mas por garantia)
        for h in list(tree_dict.keys()):
            for p in list(tree_dict[h].keys()):
                if not tree_dict[h][p]:
                    del tree_dict[h][p]
            if not tree_dict[h]:
                del tree_dict[h]

        def make_node(name, children=None, value=None, extra=None):
            node = {"name": name}
            if value is not None: node["value"] = value
            if extra is not None: node["extra"] = extra
            if children: node["children"] = children
            return node

        def build_echarts_tree(d):
            echarts_children = []
            for hto, precs in sorted(d.items()):
                prec_children = []
                all_idx_hto = []
                for prec, ws_dict in sorted(precs.items()):
                    ws_children = []
                    all_idx_prec = []
                    for ws, idx_list in sorted(ws_dict.items()):
                        key_ws = f"WS::{hto}::{prec}::{ws}"
                        add_index(key_ws, idx_list)
                        ws_children.append(make_node(
                            ws,
                            value=len(idx_list),
                            extra={"type":"ws","key":key_ws}
                        ))
                        all_idx_prec.extend(idx_list)
                    key_prec = f"PREC::{hto}::{prec}"
                    add_index(key_prec, all_idx_prec)
                    prec_children.append(make_node(
                        prec,
                        children=ws_children,
                        value=len(all_idx_prec),
                        extra={"type":"prec","key":key_prec}
                    ))
                    all_idx_hto.extend(all_idx_prec)

                key_hto = f"HTO::{hto}"
                add_index(key_hto, all_idx_hto)
                echarts_children.append(make_node(
                    hto,
                    children=prec_children,
                    value=len(all_idx_hto),
                    extra={"type":"hto","key":key_hto}
                ))

            # raiz (apenas r√≥tulo)
            root = make_node("ROOT", children=echarts_children, value=None)
            return root

        root_data = build_echarts_tree(tree_dict)

        # --- 4) Desenho da √ÅRVORE (ECharts)
        st.subheader("üåø √Årvore interativa (colaps√°vel)")
        options = {
            "tooltip": {
                "trigger": "item",
                "triggerOn": "mousemove",
                "formatter": """function(p) {
                    var v = (p.value !== undefined) ? ("<br/>Freq: " + p.value) : "";
                    return "<b>" + p.name + "</b>" + v;
                }"""
            },
            "series": [{
                "type": "tree",
                "data": [root_data],
                "left": "2%",
                "right": "20%",
                "top": "2%",
                "bottom": "2%",
                "symbol": "circle",
                "symbolSize": 10,
                "expandAndCollapse": True,
                "initialTreeDepth": int(init_depth),
                "animationDuration": 300,
                "animationDurationUpdate": 300,
                "label": {"position": "left", "verticalAlign": "middle", "align": "right", "fontSize": 12},
                "leaves": {"label": {"position": "right", "align": "left"}},
                "emphasis": {"focus": "descendant"},
                "roam": True
            }]
        }

        # IMPORTANTe: o streamlit_echarts espera um dicion√°rio de events
        event = st_echarts(
            options=options,
            height="650px",
            events={"click": "function(params) { return params; }"}
        )

        # --- 5) Drill-down ao clicar no n√≥
        st.subheader("üîé Detalhes do n√≥ selecionado")
        if event and isinstance(event, dict):
            data = event.get("data", {}) or {}
            extra = data.get("extra", {})
            key = extra.get("key")
            if key and key in node_index:
                idxs = sorted(node_index[key])
                detail = (tri_f
                          .reset_index(drop=True)
                          .iloc[idxs][["HTO","Precursor","WeakSignal","File","Paragraph","Snippet"]]
                          .copy())
                st.write(f"**N√≥:** `{event.get('name','')}` ‚Äî **linhas:** {len(detail)}")
                st.dataframe(detail, use_container_width=True)
                st.download_button(
                    "üì• Baixar CSV deste n√≥",
                    data=detail.to_csv(index=False).encode("utf-8"),
                    file_name="detalhes_no.csv",
                    mime="text/csv"
                )
            else:
                st.info("Clique em um n√≥ de **HTO**, **Precursor** ou **WeakSignal** para ver os detalhes.")

        # --- 6) Treemap alternativo (contagem 1 por ocorr√™ncia)
        st.subheader("üß© Treemap (alternativa)")
        tri_f["_one_"] = 1
        fig = px.treemap(
            tri_f,
            path=["HTO","Precursor","WeakSignal"],
            values="_one_",
            hover_data=["File","Snippet"]
        )
        st.plotly_chart(fig, use_container_width=True)


# -----------------------------------------------------------------------------
# DOWNLOAD EXCEL
# -----------------------------------------------------------------------------
st.subheader("‚¨áÔ∏è Download (Excel consolidado)")
dfs_out = {
    "WS_hits": ws_hits,
    "Precursores_hits": prec_hits,
    "Taxonomia_hits": tax_hits,
    "Relatorios_similares": sim_reports
}
st.download_button(
    "Baixar resultados (.xlsx)",
    data=to_excel_bytes(dfs_out),
    file_name=f"analise_evento_{int(time.time())}.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
)

st.caption("Use os limiares na barra lateral para equilibrar cobertura vs. precis√£o. Os artefatos s√£o carregados do reposit√≥rio p√∫blico informado.")
